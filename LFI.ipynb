{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72319b2e-41aa-4a4d-962a-568ac741f098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Starting n=100 and m=50 #####\n",
      "##### Starting N_epoch=51 epochs #####\n",
      "##### K=15 big trials, N=1000 tests per trial for inference of Z. #####\n",
      "### Start kk ###\n",
      "Epoch: 0\n",
      "mmd_value:  -0.0018405765295028687\n",
      "Statistic J:  -0.02907496504485607\n",
      "Under this trained kernel, we run N = 1000 times LFI: \n",
      "n, m= 100  50 --- P(success|Z~X):  0.698\n",
      "n, m= 100  50 --- P(success|Z~Y):  0.667\n",
      "### Start kk ###\n",
      "Epoch: 0\n",
      "mmd_value:  0.002686161547899246\n",
      "Statistic J:  0.07462821900844574\n",
      "Under this trained kernel, we run N = 1000 times LFI: \n",
      "n, m= 100  50 --- P(success|Z~X):  0.703\n",
      "n, m= 100  50 --- P(success|Z~Y):  0.651\n",
      "### Start kk ###\n",
      "Epoch: 0\n",
      "mmd_value:  0.0024238964542746544\n",
      "Statistic J:  0.060590870678424835\n",
      "Under this trained kernel, we run N = 1000 times LFI: \n",
      "n, m= 100  50 --- P(success|Z~X):  0.72\n",
      "n, m= 100  50 --- P(success|Z~Y):  0.722\n",
      "### Start kk ###\n",
      "Epoch: 0\n",
      "mmd_value:  -0.0009325165301561356\n",
      "Statistic J:  -0.02443053387105465\n",
      "Under this trained kernel, we run N = 1000 times LFI: \n",
      "n, m= 100  50 --- P(success|Z~X):  0.673\n",
      "n, m= 100  50 --- P(success|Z~Y):  0.656\n",
      "### Start kk ###\n",
      "Epoch: 0\n",
      "mmd_value:  0.0054455287754535675\n",
      "Statistic J:  0.11800167709589005\n",
      "Under this trained kernel, we run N = 1000 times LFI: \n",
      "n, m= 100  50 --- P(success|Z~X):  0.833\n",
      "n, m= 100  50 --- P(success|Z~Y):  0.796\n",
      "### Start kk ###\n",
      "Epoch: 0\n",
      "mmd_value:  -0.0017168326303362846\n",
      "Statistic J:  -0.06624697893857956\n",
      "Under this trained kernel, we run N = 1000 times LFI: \n",
      "n, m= 100  50 --- P(success|Z~X):  0.635\n",
      "n, m= 100  50 --- P(success|Z~Y):  0.642\n",
      "### Start kk ###\n",
      "Epoch: 0\n",
      "mmd_value:  0.0031148516573011875\n",
      "Statistic J:  0.09064872562885284\n",
      "Under this trained kernel, we run N = 1000 times LFI: \n",
      "n, m= 100  50 --- P(success|Z~X):  0.77\n",
      "n, m= 100  50 --- P(success|Z~Y):  0.743\n",
      "### Start kk ###\n",
      "Epoch: 0\n",
      "mmd_value:  0.00044584833085536957\n",
      "Statistic J:  0.007476452272385359\n",
      "Under this trained kernel, we run N = 1000 times LFI: \n",
      "n, m= 100  50 --- P(success|Z~X):  0.687\n",
      "n, m= 100  50 --- P(success|Z~Y):  0.701\n",
      "### Start kk ###\n",
      "Epoch: 0\n",
      "mmd_value:  -0.002278495579957962\n",
      "Statistic J:  -0.05439804494380951\n",
      "Under this trained kernel, we run N = 1000 times LFI: \n",
      "n, m= 100  50 --- P(success|Z~X):  0.744\n",
      "n, m= 100  50 --- P(success|Z~Y):  0.725\n",
      "### Start kk ###\n",
      "Epoch: 0\n",
      "mmd_value:  -0.011148646473884583\n",
      "Statistic J:  -0.19187702238559723\n",
      "Under this trained kernel, we run N = 1000 times LFI: \n",
      "n, m= 100  50 --- P(success|Z~X):  0.643\n",
      "n, m= 100  50 --- P(success|Z~Y):  0.679\n",
      "### Start kk ###\n",
      "Epoch: 0\n",
      "mmd_value:  0.003736947663128376\n",
      "Statistic J:  0.10175498574972153\n",
      "Under this trained kernel, we run N = 1000 times LFI: \n",
      "n, m= 100  50 --- P(success|Z~X):  0.723\n",
      "n, m= 100  50 --- P(success|Z~Y):  0.704\n",
      "### Start kk ###\n",
      "Epoch: 0\n",
      "mmd_value:  0.001082326052710414\n",
      "Statistic J:  0.04496138170361519\n",
      "Under this trained kernel, we run N = 1000 times LFI: \n",
      "n, m= 100  50 --- P(success|Z~X):  0.649\n",
      "n, m= 100  50 --- P(success|Z~Y):  0.638\n",
      "### Start kk ###\n",
      "Epoch: 0\n",
      "mmd_value:  0.010296723805367947\n",
      "Statistic J:  0.17021328210830688\n",
      "Under this trained kernel, we run N = 1000 times LFI: \n",
      "n, m= 100  50 --- P(success|Z~X):  0.654\n",
      "n, m= 100  50 --- P(success|Z~Y):  0.666\n",
      "### Start kk ###\n",
      "Epoch: 0\n",
      "mmd_value:  0.007422948256134987\n",
      "Statistic J:  0.1689320206642151\n",
      "Under this trained kernel, we run N = 1000 times LFI: \n",
      "n, m= 100  50 --- P(success|Z~X):  0.641\n",
      "n, m= 100  50 --- P(success|Z~Y):  0.645\n",
      "### Start kk ###\n",
      "Epoch: 0\n",
      "mmd_value:  -0.005357876420021057\n",
      "Statistic J:  -0.24944865703582764\n",
      "Under this trained kernel, we run N = 1000 times LFI: \n",
      "n, m= 100  50 --- P(success|Z~X):  0.697\n",
      "n, m= 100  50 --- P(success|Z~Y):  0.66\n",
      "END\n",
      "Success rates:\n",
      "[0.698]\n",
      "[0.68633333]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "from sklearn.utils import check_random_state\n",
    "from utils import MatConvert, MMDu, TST_MMD_u, mmd2_permutations, MMD_General, MMD_LFI_std\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from Data_gen import *\n",
    "\n",
    "class ModelLatentF(torch.nn.Module):\n",
    "    \"\"\"Latent space for both domains.\"\"\"\n",
    "    \"\"\" Dense Net with w=50, d=4, ~relu, in=2, out=50 \"\"\"\n",
    "    def __init__(self, x_in, H, x_out):\n",
    "        \"\"\"Init latent features.\"\"\"\n",
    "        super(ModelLatentF, self).__init__()\n",
    "        self.restored = False\n",
    "        self.latent = torch.nn.Sequential(\n",
    "            torch.nn.Linear(x_in, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, x_out, bias=True),\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward the LeNet.\"\"\"\n",
    "        fealant = self.latent(input)\n",
    "        return fealant\n",
    "\n",
    "def LFI_plot(n_list, title=\"LFI_with_Blob\" ,path='./data/', with_error_bar=True):\n",
    "    Results_list = []\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ZX_success = np.zeros(len(n_list))\n",
    "    ZY_success = np.zeros(len(n_list))\n",
    "    ZX_err = np.zeros(len(n_list))\n",
    "    ZY_err = np.zeros(len(n_list))\n",
    "    for i,n in enumerate(n_list):\n",
    "        Results_list.append(np.load(path+'LFI_%d.npy'%n))\n",
    "        ZX_success[i] = np.mean(Results_list[-1][0,:])\n",
    "        ZY_success[i] = np.mean(Results_list[-1][1,:])\n",
    "        ZX_err[i] = np.std(Results_list[-1][0,:])\n",
    "        ZY_err[i] = np.std(Results_list[-1][1,:])\n",
    "    if with_error_bar==True:\n",
    "        plt.errorbar(n_list, ZX_success, yerr=ZX_err, label='Z~X')\n",
    "        plt.errorbar(n_list, ZY_success, yerr=ZY_err, label='Z~Y')\n",
    "    else:\n",
    "        plt.plot(n_list, ZX_success, label='Z~X')\n",
    "        plt.plot(n_list, ZY_success, label='Z~Y')\n",
    "    print('Success rates:')\n",
    "    print(ZX_success)\n",
    "    print(ZY_success)\n",
    "    plt.xlabel(\"n samples\", fontsize=20)\n",
    "    plt.ylabel(\"P(success)\", fontsize=20)\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.savefig(title+'.png')\n",
    "    plt.close()\n",
    "    return fig\n",
    "\n",
    "def mmdG(X, Y, model_u, n, m, sigma, sigma0_u, device, dtype, ep):\n",
    "    S = np.concatenate((X, Y), axis=0)\n",
    "    S = MatConvert(S, device, dtype)\n",
    "    Fea = model_u(S)\n",
    "    n = X.shape[0]\n",
    "    m = Y.shape[0]\n",
    "    return MMD_General(Fea, n, m, S, sigma, sigma0_u, ep)\n",
    "\n",
    "def train_d(n_list, m_list, N_per=100, title='Default', learning_rate=5e-4, K=15, N=1000, \n",
    "            N_epoch=51, print_every=100, batch_size=50, test_on_new_sample=True, SGD=True, LfI=False):  \n",
    "  # Setup seeds\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    dtype = torch.float\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    x_in = 2 # number of neurons in the input layer, i.e., dimension of data\n",
    "    H = 50 # number of neurons in the hidden layer\n",
    "    x_out = 50 # number of neurons in the output layer\n",
    "    N_f = float(N) # number of test sets (float)\n",
    "    parameters={'n_list':n_list,\n",
    "                'm_list':m_list,\n",
    "                'N_epoch':N_epoch,\n",
    "                'N_per':N_per,\n",
    "                'K':K,\n",
    "                'N':N,}\n",
    "    with open('./data/PARAMETERS_'+title, 'wb') as pickle_file:\n",
    "        pickle.dump(parameters, pickle_file)\n",
    "    # Generate variance and co-variance matrix of Q\n",
    "    sigma_mx_2_standard = np.array([[0.03, 0], [0, 0.03]])\n",
    "    sigma_mx_2 = np.zeros([9,2,2])\n",
    "    for i in range(9):\n",
    "        sigma_mx_2[i] = sigma_mx_2_standard\n",
    "        if i < 4:\n",
    "            sigma_mx_2[i][0 ,1] = -0.02 - 0.002 * i\n",
    "            sigma_mx_2[i][1, 0] = -0.02 - 0.002 * i\n",
    "        if i==4:\n",
    "            sigma_mx_2[i][0, 1] = 0.00\n",
    "            sigma_mx_2[i][1, 0] = 0.00\n",
    "        if i>4:\n",
    "            sigma_mx_2[i][1, 0] = 0.02 + 0.002 * (i-5)\n",
    "            sigma_mx_2[i][0, 1] = 0.02 + 0.002 * (i-5)\n",
    "\n",
    "    for i in range(len(n_list)):\n",
    "        n=n_list[i]\n",
    "        m=m_list[i]\n",
    "        print(\"##### Starting n=%d and m=%d #####\"%(n, m))\n",
    "        print(\"##### Starting N_epoch=%d epochs #####\"%(N_epoch))\n",
    "        print(\"##### K=%d big trials, N=%d tests per trial for inference of Z. #####\"%(K,N))\n",
    "        Results = np.zeros([2, K])\n",
    "        J_star_u = np.zeros([K, N_epoch])\n",
    "        ep_OPT = np.zeros([K])\n",
    "        s_OPT = np.zeros([K])\n",
    "        s0_OPT = np.zeros([K])\n",
    "        \n",
    "        for kk in range(K):\n",
    "            print(\"### Start kk ###\")\n",
    "            if not SGD:\n",
    "                batch_size=n\n",
    "            else:\n",
    "                batches=n//batch_size\n",
    "                n=batches*batch_size #round up\n",
    "                m=(m//batches)*batches\n",
    "                batch_m=m//batches\n",
    "            \n",
    "            X, Y = sample_blobs_Q(n, sigma_mx_2)\n",
    "            Z, _ = sample_blobs_Q(m, sigma_mx_2)\n",
    "            total_S=[(X[i*batch_size:i*batch_size+batch_size], Y[i*batch_size:i*batch_size+batch_size]) for i in range(batches)]\n",
    "            #total_S=[MatConvert(np.concatenate((X, Y), axis=0), device, dtype) for (X, Y) in total_S]\n",
    "            total_Z=[Z[i*batch_m:i*batch_m+batch_m] for i in range(batches)]\n",
    "            model_u = ModelLatentF(x_in, H, x_out).cuda()\n",
    "            epsilonOPT = MatConvert(np.random.rand(1) * (10 ** (-10)), device, dtype)\n",
    "            epsilonOPT.requires_grad = True\n",
    "            sigmaOPT = MatConvert(np.sqrt(np.random.rand(1) * 0.3), device, dtype)\n",
    "            sigmaOPT.requires_grad = True\n",
    "            sigma0OPT = MatConvert(np.sqrt(np.random.rand(1) * 0.002), device, dtype)\n",
    "            sigma0OPT.requires_grad = True\n",
    "            # Setup optimizer for training deep kernel\n",
    "            optimizer_u = torch.optim.Adam(list(model_u.parameters())+[epsilonOPT]+[sigmaOPT]+[sigma0OPT], lr=learning_rate)\n",
    "            # Train deep kernel to maximize test power\n",
    "            for t in range(N_epoch):\n",
    "                # Compute epsilon, sigma and sigma_0\n",
    "                for ind in range(batches):\n",
    "                    x, y=total_S[ind] #minibatches\n",
    "                    z=total_Z[ind]\n",
    "                    ep = torch.exp(epsilonOPT)/(1+torch.exp(epsilonOPT))\n",
    "                    sigma = sigmaOPT ** 2\n",
    "                    sigma0_u = sigma0OPT ** 2\n",
    "                    if not LfI:\n",
    "                        S=MatConvert(np.concatenate((x, y), axis=0), device, dtype)\n",
    "                        modelu_output = model_u(S)\n",
    "                        TEMP = MMDu(modelu_output, batch_size, S, sigma, sigma0_u, ep)\n",
    "                        mmd_value_temp = -1 * TEMP[0]\n",
    "                        mmd_std_temp = torch.sqrt(TEMP[1]+10**(-8)) #this is std\n",
    "                        STAT_u = torch.div(mmd_value_temp, mmd_std_temp)\n",
    "                    else:\n",
    "                        S=MatConvert(np.concatenate(([x, y, z]), axis=0), device, dtype)\n",
    "                        Fea=model_u(S)\n",
    "                        #mmd_value_temp = torch.square(mmdG(x, z, model_u, batch_size, batch_m, sigma, sigma0_u, device, dtype, ep)-mmdG(y, z, model_u, batch_size, batch_m, sigma, sigma0_u, device, dtype, ep))\n",
    "                        #The above is correct form but very slow.\n",
    "                        lfi_temp=MMD_LFI_std(Fea, S, batch_size, batch_m,  sigma, sigma0_u, ep)\n",
    "                        #TODO: compute std\n",
    "                        mmd_squared_temp=torch.square(lfi_temp[0])\n",
    "                        mmd_var_temp=torch.nn.ReLU(lfi_temp[1])\n",
    "                        STAT_u = torch.sub(mmd_squared_temp, mmd_var_temp, alpha=1.0)\n",
    "                    J_star_u[kk, t] = STAT_u.item()\n",
    "                    optimizer_u.zero_grad()\n",
    "                    STAT_u.backward(retain_graph=True)\n",
    "                    optimizer_u.step()\n",
    "                # Print MMD, std of MMD and J\n",
    "                if t % print_every == 0:\n",
    "                    print('Epoch:', t)\n",
    "                    print(\"mmd_value: \", -1 * mmd_value_temp.item()) \n",
    "                          #\"mmd_std: \", mmd_std_temp.item(), \n",
    "                    print(\"Statistic J: \", -1 * STAT_u.item())\n",
    "            ep_OPT[kk] = ep.item()\n",
    "            s_OPT[kk] = sigma.item()\n",
    "            s0_OPT[kk] = sigma0_u.item()\n",
    "            '''\n",
    "            #testing how model behaves on untrained data\n",
    "            print('TEST OUR MODEL ON NEW SET OF DATA:')            \n",
    "            X1, Y1 = sample_blobs_Q(n, sigma_mx_2)\n",
    "            with torch.torch.no_grad():\n",
    "                S1 = np.concatenate((X1, Y1), axis=0)\n",
    "                S1 = MatConvert(S1, device, dtype)\n",
    "                modelu_output = model_u(S1)\n",
    "                TEMP = MMDu(modelu_output, n, S1, sigma, sigma0_u, ep)\n",
    "                mmd_value_temp = -1 * TEMP[0]\n",
    "                mmd_std_temp = torch.sqrt(TEMP[1]+10**(-8))\n",
    "                STAT_u = torch.div(mmd_value_temp, mmd_std_temp)\n",
    "                if True:\n",
    "                    print(\"TEST mmd_value: \", -1 * mmd_value_temp.item()) \n",
    "                    print(\"TEST Statistic J: \", -1 * STAT_u.item())\n",
    "            #print('epsilon:', ep)\n",
    "            #print('sigma:  ', sigma)\n",
    "            #print('sigma0: ', sigma0_u) \n",
    "            '''\n",
    "            H_u = np.zeros(N) \n",
    "            print(\"Under this trained kernel, we run N = %d times LFI: \"%N)\n",
    "            \n",
    "            ##### Z~X #####\n",
    "            for k in range(N):\n",
    "                if test_on_new_sample:\n",
    "                    X, Y = sample_blobs_Q(n, sigma_mx_2)\n",
    "                Z, _ = sample_blobs_Q(m, sigma_mx_2)\n",
    "                # Run MMD on generated data\n",
    "                mmd_XZ = mmdG(X, Z, model_u, n, m, sigma, sigma0_u, device, dtype, ep)[0]\n",
    "                mmd_YZ = mmdG(Y, Z, model_u, n, m, sigma, sigma0_u, device, dtype, ep)[0]\n",
    "                H_u[k] = mmd_XZ<mmd_YZ    \n",
    "            print(\"n, m=\",str(n)+str('  ')+str(m),\"--- P(success|Z~X): \", H_u.sum()/N_f)\n",
    "            Results[0, kk] = H_u.sum() / N_f\n",
    "\n",
    "            ##### Z~Y #####\n",
    "            for k in range(N):\n",
    "                if test_on_new_sample:\n",
    "                    X, Y = sample_blobs_Q(n, sigma_mx_2)\n",
    "                _, Z = sample_blobs_Q(m, sigma_mx_2)\n",
    "                mmd_XZ = mmdG(X, Z, model_u, n, m, sigma, sigma0_u, device, dtype, ep)[0]\n",
    "                mmd_YZ = mmdG(Y, Z, model_u, n, m, sigma, sigma0_u, device, dtype, ep)[0]\n",
    "                H_u[k] = mmd_XZ>mmd_YZ\n",
    "            print(\"n, m=\",str(n)+str('  ')+str(m),\"--- P(success|Z~Y): \", H_u.sum()/N_f)\n",
    "            Results[1, kk] = H_u.sum() / N_f\n",
    "        ##### Save np #####\n",
    "        print(\"END\")\n",
    "        np.save('./data/LFI_'+str(n),Results) \n",
    "    ####Plotting    \n",
    "    LFI_plot(n_list, title=title)\n",
    "\n",
    "def train_O(n_list, m_list):\n",
    "    #Trains optimized Gaussian Kernel Length\n",
    "    #implemented in DK-TST\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_list = 50*np.array(range(12,13)) # number of samples in per mode\n",
    "    m_list = 10*np.array(range(4,5))\n",
    "    try:\n",
    "        title=sys.argv[1]\n",
    "    except:\n",
    "        title='untitled_run'\n",
    "    train_d([100], [50], title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4711536-f59f-4617-9e1d-37bfd928ba92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502d8781-2853-4d15-8e50-63444d04910e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
