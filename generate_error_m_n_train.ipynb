{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: value of $r$ is assigned in block [24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "from utils_old import *\n",
    "import utils\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from numba import cuda\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import pyroc\n",
    "import pandas as pd\n",
    "import gc\n",
    "from IPython.display import clear_output\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"  \n",
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float32\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 300\n",
    "out= 100\n",
    "L = 1\n",
    "class DN(torch.nn.Module):\n",
    "    def __init__(self, H=300, out=100):\n",
    "        super(DN, self).__init__()\n",
    "        self.restored = False\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, out, bias=True),\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        output = self.model(input)\n",
    "        return output\n",
    "\n",
    "class another_DN(torch.nn.Module):\n",
    "    def __init__(self, H=300, out=100):\n",
    "        super(another_DN, self).__init__()\n",
    "        self.restored = False\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28, H, bias=True),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(H, 28, bias=True),\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        output = self.model(input) + input\n",
    "        return output\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, H=300, layers = 5):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.restored = False\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, 1, bias=True),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "        if layers == 6:\n",
    "            self.model = torch.nn.Sequential(\n",
    "                torch.nn.Linear(28, H, bias=True),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(H, H, bias=True),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(H, H, bias=True),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(H, H, bias=True),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(H, H, bias=True),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(H, 1, bias=True),\n",
    "                torch.nn.Sigmoid(),\n",
    "            )\n",
    "    def forward(self, input):\n",
    "        output = self.model(input)\n",
    "        return output\n",
    "dataset = np.load('HIGGS.npy')\n",
    "dataset_P = dataset[dataset[:,0]==0][:, 1:] # background (5829122, 28)\n",
    "dataset_Q = dataset[dataset[:,0]==1][:, 1:] # signal     (5170877, 28)\n",
    "del dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creat class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_PQ(X_test, Y_test, model, another_model, epsilonOPT, sigmaOPT, sigma0OPT, cst,\n",
    "                n_test, Samples, batch_size = 5000, If_n_large_MonteCarlo = 1000, test_batch_size=20000):\n",
    "    '''\n",
    "    Input: \n",
    "        n_eval: n_eval in our paper, then we generate $X^{eval}$ and $Y^{eval}$ used to compute MMD.\n",
    "        Samples: we generate 'Samples' samples from P and Q, then compute their scores f(sample;X_eval,Y_eval)\n",
    "    Output:\n",
    "        X_eval, Y_eval: generate from P and Q, each with size (n_eval, 28)\n",
    "        P_scores, Q_scores: scores of samples from P and Q, each with size (Samples,)\n",
    "        EKxx, EKyy, EKxy: expectation of K(X,X), K(Y,Y), K(X,Y), where X~P_X, Y~P_Y. Then the gamma in our paper can be computed from these three values.\n",
    "    '''\n",
    "    # X_test = dataset_P[np.random.choice(dataset_P.shape[0], n_test, replace=False)]\n",
    "    # Y_test = dataset_Q[np.random.choice(dataset_Q.shape[0], n_test, replace=False)]\n",
    "    EKxx, EKyy, EKxy = compute_gamma(X_test, Y_test, model, another_model, epsilonOPT, sigmaOPT, sigma0OPT, cst, MonteCarlo=If_n_large_MonteCarlo)\n",
    "    batches = (Samples-1)//batch_size + 1\n",
    "    P_scores = np.zeros(Samples)\n",
    "    Q_scores = np.zeros(Samples)\n",
    "    for i in trange(batches):\n",
    "        remain = batch_size\n",
    "        if i==batches-1:\n",
    "            remain = Samples - batch_size*(batches-1)\n",
    "        S_hand = np.concatenate((dataset_P[np.random.choice(dataset_P.shape[0], remain, replace=False)],\n",
    "                    dataset_Q[np.random.choice(dataset_Q.shape[0], remain, replace=False)]), axis=0)\n",
    "        S_hand = MatConvert(S_hand, device, dtype)\n",
    "        PQhat_hand = compute_score_func(S_hand, X_test, Y_test, \n",
    "                    model, another_model, epsilonOPT, sigmaOPT, sigma0OPT, cst,\n",
    "                    M = test_batch_size)\n",
    "        PQhat_hand = PQhat_hand.cpu().detach().numpy()\n",
    "        P_scores[i*batch_size: i*batch_size+remain] = PQhat_hand[:remain]\n",
    "        Q_scores[i*batch_size: i*batch_size+remain] = PQhat_hand[remain:]\n",
    "        del S_hand, PQhat_hand\n",
    "        gc.collect()\n",
    "    return X_test, Y_test, P_scores, Q_scores, EKxx, EKyy, EKxy\n",
    "\n",
    "class PQ_data():\n",
    "    '''\n",
    "    Define a class to compute the p-value, the Type I/II error.\n",
    "    '''\n",
    "    def __init__(self, P_scores, Q_scores, EKxx, EKyy, EKxy):\n",
    "        self.P_scores = P_scores\n",
    "        self.Q_scores = Q_scores\n",
    "        self.P_mean = np.mean(P_scores)\n",
    "        self.P_std = np.std(P_scores)\n",
    "        self.Q_mean = np.mean(Q_scores)\n",
    "        self.Q_std = np.std(Q_scores)\n",
    "        self.EKxx = EKxx\n",
    "        self.EKyy = EKyy\n",
    "        self.EKxy = EKxy\n",
    "    def pval_T_m_in_sigma(self, pi, m, use_gaussian, MonteCarlo):\n",
    "        '''\n",
    "        Compute the expected significance of discovery.\n",
    "        Input:\n",
    "            pi: the strength of the mixture in our paper.\n",
    "            m: the number of samples, i.e. the size of Z in our paper.\n",
    "            use_gaussian: if use Gaussian Approximation, then use_gaussian = True, else use_gaussian = False.\n",
    "            MonteCarlo: when use_gaussian = False, we use Monte Carlo to approximate the expectation.\n",
    "        Output:\n",
    "            p: the expected significance of discovery (in units of Gaussian sigma)\n",
    "        '''\n",
    "        T = pi*self.Q_mean + (1-pi)*self.P_mean\n",
    "        P_scores = self.P_scores\n",
    "        mean = self.P_mean\n",
    "        std = self.P_std\n",
    "        if m==1:\n",
    "            p = np.mean(P_scores > T)\n",
    "            p = -scipy.stats.norm.ppf(p)\n",
    "            self.p = p\n",
    "            return p\n",
    "        if use_gaussian:\n",
    "            p = (T-mean)/std*np.sqrt(m)\n",
    "        else:\n",
    "            T_mix_MonteCarlo_list = np.zeros(MonteCarlo)\n",
    "            for i in range(MonteCarlo):\n",
    "                idx = np.random.choice(P_scores.shape[0], m, replace=False)\n",
    "                T_mix_MonteCarlo_list[i] = np.mean(P_scores[idx])\n",
    "            p = np.mean(T_mix_MonteCarlo_list > T)\n",
    "            p = -scipy.stats.norm.ppf(p)\n",
    "        self.p = p\n",
    "        return p\n",
    "    def type_1_error_H0(self, pi, m, use_gaussian, MonteCarlo):\n",
    "        '''\n",
    "        Compute the estimated Type I error.\n",
    "        Input:\n",
    "            pi: the strength of the mixture in our paper.\n",
    "            m: the number of samples, i.e. the size of Z in our paper.\n",
    "            use_gaussian: if use Gaussian Approximation, then use_gaussian = True, else use_gaussian = False.\n",
    "            MonteCarlo: when use_gaussian = False, we use Monte Carlo to approximate the expectation.\n",
    "        Output:\n",
    "            type_1_error: the estimated Type I error.   \n",
    "        '''\n",
    "        P_scores = self.P_scores\n",
    "        Q_scores = self.Q_scores\n",
    "        mean = self.P_mean\n",
    "        std = self.P_std\n",
    "        P_mean = self.P_mean\n",
    "        P_std = self.P_std\n",
    "        Q_mean = self.Q_mean\n",
    "        gamma = self.EKxx*(pi/2-1) + self.EKxy*(1-pi) + self.EKyy*(pi/2)\n",
    "        #gamma = (pi/2)*Q_mean + (1-pi/2)*P_mean\n",
    "        self.gamma = gamma\n",
    "        if m==1:\n",
    "            type_1_error = np.mean(P_scores > gamma)\n",
    "            self.type_1_error = type_1_error\n",
    "            return type_1_error\n",
    "        if use_gaussian:\n",
    "            type_1_error = 1-scipy.stats.norm.cdf((gamma-mean)/std*np.sqrt(m))\n",
    "        else:\n",
    "            MonteCarlo_list = np.zeros(MonteCarlo)\n",
    "            for i in range(MonteCarlo):\n",
    "                idx = np.random.choice(P_scores.shape[0], m, replace=False)\n",
    "                MonteCarlo_list[i] = np.mean(P_scores[idx])\n",
    "            type_1_error = np.mean(MonteCarlo_list > gamma)\n",
    "            del MonteCarlo_list\n",
    "            gc.collect()\n",
    "        self.type_1_error = type_1_error\n",
    "        return type_1_error\n",
    "    def type_2_error_H1(self, pi, m, use_gaussian, MonteCarlo):\n",
    "        '''\n",
    "        Compute the estimated Type II error.\n",
    "        Input:\n",
    "            pi: the strength of the mixture in our paper.\n",
    "            m: the number of samples, i.e. the size of Z in our paper.\n",
    "            use_gaussian: if use Gaussian Approximation, then use_gaussian = True, else use_gaussian = False.\n",
    "            MonteCarlo: when use_gaussian = False, we use Monte Carlo to approximate the expectation.\n",
    "        Output:\n",
    "            type_2_error: the estimated Type II error.              \n",
    "        '''\n",
    "        P_scores = self.P_scores\n",
    "        Q_scores = self.Q_scores\n",
    "        P_mean = self.P_mean\n",
    "        P_std = self.P_std\n",
    "        Q_mean = self.Q_mean\n",
    "        Q_std = self.Q_std\n",
    "        gamma = self.EKxx*(pi/2-1) + self.EKxy*(1-pi) + self.EKyy*(pi/2)\n",
    "        #gamma = (pi/2)*Q_mean + (1-pi/2)*P_mean\n",
    "        self.gamma = gamma\n",
    "        if m==1:\n",
    "            type_2_error = np.mean(Q_scores < gamma)\n",
    "            self.type_2_error = type_2_error\n",
    "            return type_2_error\n",
    "        if use_gaussian:\n",
    "            mean = Q_mean*pi + P_mean*(1-pi)\n",
    "            std = np.sqrt(pi*Q_std**2 + (1-pi)*P_std**2 + pi*(1-pi)*(P_mean-Q_mean)**2)\n",
    "            type_2_error = scipy.stats.norm.cdf((gamma-mean)/std*np.sqrt(m))\n",
    "        else:\n",
    "            MonteCarlo_list = np.zeros(MonteCarlo)\n",
    "            for i in range(MonteCarlo):\n",
    "                Signals_idx = np.random.choice(Q_scores.shape[0], int(m*pi), replace=False)\n",
    "                Backgrounds_idx = np.random.choice(P_scores.shape[0], int(m*(1-pi)), replace=False)\n",
    "                MonteCarlo_list[i] = np.mean(np.concatenate((Q_scores[Signals_idx], P_scores[Backgrounds_idx])))\n",
    "            type_2_error = np.mean(MonteCarlo_list < gamma)\n",
    "            del MonteCarlo_list\n",
    "            gc.collect()\n",
    "        self.type_2_error = type_2_error\n",
    "        return type_2_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate distribution of scores for a given $n_{train}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = np.array([2000000, 1600000, 1300000, 1000000, 700000, 400000, 200000, 100000, 50000, 30000, 10000, 6000, 3000, 1000, 600, 300])\n",
    "ns = np.array([1600000, 1300000, 1000000, 700000, 400000, 200000, 100000, 50000, 30000, 10000, 8000, 6000, 4500, 3000, 2000, 1000])\n",
    "pi=0.1\n",
    "gc.collect()\n",
    "torch. cuda. empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given $r$ below, and some $n$, we use the checkpoint path = \"./Res_Net/checkpoint $(n+r)$/0/\"  \n",
    "To get a smooth plot, one should change the value of $r$ from 0 to 9, and rerun the whole .ipynb 10 times manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0 # For a given n, we use the checkpoint of the r-th trained network\n",
    "repeats_per_trained_model = 50 # for each trained network, we repeat the experiment this many times\n",
    "My_class_list = [[]]*repeats_per_trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(repeats_per_trained_model):\n",
    "    for i,n in enumerate(ns):\n",
    "        print(k,n)\n",
    "        path = './Res_Net/checkpoint%d/0/'%(n+r)\n",
    "        model = DN(300, 100).cuda()\n",
    "        another_model = another_DN(300, 100).cuda()\n",
    "        model,another_model,epsilonOPT,sigmaOPT,sigma0OPT,cst = load_model(model, another_model, path)\n",
    "\n",
    "        n_train = n\n",
    "        n_test = min(n,20000)\n",
    "        n_eval = 20000\n",
    "        idx = np.random.choice(dataset_P.shape[0]-n_train, n_eval+n_test, replace=False) + n_train\n",
    "        idy = np.random.choice(dataset_Q.shape[0]-n_train, n_eval+n_test, replace=False) + n_train\n",
    "        X_test = dataset_P[np.random.choice(n_train, n_test, replace=False)]\n",
    "        Y_test = dataset_Q[np.random.choice(n_train, n_test, replace=False)]\n",
    "        X_eval = dataset_P[idx][n_test:n_test+n_eval]\n",
    "        Y_eval = dataset_Q[idy][n_test:n_test+n_eval]\n",
    "        X_test = MatConvert(X_test, device=device, dtype=dtype)\n",
    "        Y_test = MatConvert(Y_test, device=device, dtype=dtype)\n",
    "        X_eval = MatConvert(X_eval, device=device, dtype=dtype)\n",
    "        Y_eval = MatConvert(Y_eval, device=device, dtype=dtype)\n",
    "        X_test_eval = X_test\n",
    "        Y_test_eval = Y_test\n",
    "\n",
    "        # p_soft = utils.get_pval_at_once(X_test, Y_test, X_test_eval, Y_test_eval, X_eval, Y_eval,\n",
    "        #                     model,another_model,epsilonOPT,sigmaOPT,sigma0OPT,cst,\n",
    "        #                     batch_size = 5000,\n",
    "        #                     norm_or_binom=True)\n",
    "        batch_size = 10000\n",
    "        Samples = 20000\n",
    "        If_n_large_MonteCarlo = 10\n",
    "        X_test, Y_test, P_scores, Q_scores, EKxx, EKyy, EKxy = generate_PQ(X_test, Y_test, model, another_model, epsilonOPT, sigmaOPT, sigma0OPT, cst,\n",
    "                                                                        n_test, Samples=20000, batch_size=5000, \n",
    "                                                                        If_n_large_MonteCarlo=10,\n",
    "                                                                        test_batch_size=min(n,20000))\n",
    "        My_class  = PQ_data(P_scores, Q_scores, EKxx, EKyy, EKxy)\n",
    "        My_class_list[k].append(My_class)\n",
    "        del My_class\n",
    "        gc.collect()\n",
    "        clear_output()\n",
    "        print(k,n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate error for different $m$\n",
    "Note: one can adjust $ms$, the list of $m$ to calculate. But please make sure that generate_error_m_n_train.ipynb and plot_error_m_n_train.ipynb are using the same $ms$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My_class_list100 = My_class_list\n",
    "# error_mat = type1_mat+type2_mat\n",
    "# Error_m_ntr = np.mean(type1_mat+type2_mat, axis=2)\n",
    "# Error_m_ntr = error_mat[:,:,1]\n",
    "MonteCarlo = 1000\n",
    "step=5/49 # this can be changed\n",
    "new_step = 3*step # this can be changed\n",
    "ms = 10**np.linspace(0, 5, 50) # this can be changed\n",
    "ms = ms.astype(int)\n",
    "type1_mat_M = np.zeros((len(ns), len(ms), repeats_per_trained_model))\n",
    "type2_mat_M = np.zeros((len(ns), len(ms), repeats_per_trained_model))\n",
    "Error_m_ntr_M = np.zeros((len(ns), len(ms)))\n",
    "type1_mat_G = np.zeros((len(ns), len(ms), repeats_per_trained_model))\n",
    "type2_mat_G = np.zeros((len(ns), len(ms), repeats_per_trained_model))\n",
    "Error_m_ntr_G = np.zeros((len(ns), len(ms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(repeats_per_trained_model):\n",
    "    for i,n in enumerate(ns):\n",
    "        My_class = My_class_list[k][i]\n",
    "        #My_class_list[k].append(My_class)\n",
    "        MonteCarlo = 1000\n",
    "        for j in range(len(ms)):\n",
    "            print(i,j,k)\n",
    "            m = ms[j]\n",
    "            #P_m_ntr[i,j] += p_soft*11/np.sqrt(1100) * pi * np.sqrt(m)\n",
    "            # type1 = My_class.type_1_error_H0(pi, m, use_gaussian=True, MonteCarlo=MonteCarlo)\n",
    "            # type2 =  My_class.type_2_error_H1(pi, m, use_gaussian=True, MonteCarlo=MonteCarlo)\n",
    "            # type1_mat_G[i,j,k] = type1\n",
    "            # type2_mat_G[i,j,k] = type2\n",
    "            # Error_m_ntr_G[i,j] += (type2 + type1)\n",
    "            if m< 1000:\n",
    "                type1 = My_class.type_1_error_H0(pi, m, use_gaussian=False, MonteCarlo=MonteCarlo) \n",
    "                type2 =  My_class.type_2_error_H1(pi, m, use_gaussian=False, MonteCarlo=MonteCarlo)\n",
    "                type1_mat_M[i,j,k] = type1\n",
    "                type2_mat_M[i,j,k] = type2\n",
    "                Error_m_ntr_M[i,j] += (type2 + type1)\n",
    "            else:\n",
    "                type1 = My_class.type_1_error_H0(pi, m, use_gaussian=True, MonteCarlo=MonteCarlo)\n",
    "                type2 =  My_class.type_2_error_H1(pi, m, use_gaussian=True, MonteCarlo=MonteCarlo)\n",
    "                type1_mat_M[i,j,k] = type1\n",
    "                type2_mat_M[i,j,k] = type2\n",
    "                Error_m_ntr_M[i,j] += (type2 + type1)\n",
    "                \n",
    "        del My_class\n",
    "        gc.collect()\n",
    "        clear_output()\n",
    "#P_m_ntr = P_m_ntr/repeats\n",
    "# Error_m_ntr = Error_m_ntr/repeats\n",
    "Error_m_ntr_M = Error_m_ntr_M/repeats_per_trained_model\n",
    "Error_m_ntr_G = Error_m_ntr_G/repeats_per_trained_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_str = str(r)\n",
    "np.save('ns'+r_str, ns)\n",
    "np.save('ms'+r_str, ms)\n",
    "np.save('Error_m_ntr_M'+r_str, Error_m_ntr_M)\n",
    "# np.save('Error_m_ntr_G'+r_str, Error_m_ntr_M)\n",
    "# np.save('type1_mat_M'+r_str, type1_mat_M)\n",
    "# np.save('type2_mat_M'+r_str, type2_mat_M)\n",
    "# np.save('type1_mat_G'+r_str, type1_mat_M)\n",
    "# np.save('type2_mat_G'+r_str, type2_mat_M)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(5,4))\n",
    "# plt.contourf(np.log10(ns), np.log10(ms), Error_m_ntr_M.T, levels=20)\n",
    "# plt.colorbar()\n",
    "# plt.xlabel('$lg(n)$')\n",
    "# plt.ylabel('lg(m)')\n",
    "# plt.title('Type I error + Type II error (this is not the formal plot)')\n",
    "\n",
    "# #plt.suptitle(r'Fix kernel ($n_{train}=1.3e6$), Use Gaussian Approx='+str(Use_Gaussian)+', $\\pi$='+str(pi))\n",
    "# # plt.savefig('./paper/tradeoff_type.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LFI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Jun  1 2022, 11:38:51) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3398c4b5aa1e55195b0bb96b4f8fc5c3f4a0ffb5794362f58b6653d525fb08f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
