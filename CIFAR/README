This folder contains relevant tests for the CIFAR experiments in learning deep kernels for likelihood free hypothesis testing (LFHT).

Executing:
The files in code_submission are corresponding to each of the benchmarks we run. They should be able to directly ran without extra arguments with proper dataset, as detailed in the next paragraph. 
The relevant helper functions are included in utils.py. This includes training objectives and how to compute the MMD_u distances. 
To run a script, use Python3 scriptname.py and no extra inputs will be required. We assume standard Torch packages and CUDA availability for all our scripts, and require no other overheads.

Datasets:
To run the code, the corresponding datasets are needed: one from CIFAR10 which can be readily downloaded online, and one from DDPM (https://huggingface.co/google/ddpm-cifar10-32) which can be generated by the DDPM_GAN.py file, which outputs around 200 images in one hour using a NVIDIA RTX 3080 Ti. In our experiments, "../data/cifar_data.npy" has shape (50000, 3, 32, 32), and "../Diffusion/ddpm_generated_images2.npy" has shape (13140, 32, 32, 3). Although running the code on less data is certainly possible.

Outputs:
Scripts with .py prints to st out with specified names in the following forms:
1. P(max|z~x) this is the error when thresholding at zero, as part (c) in the main text's figure 2
2. P(95|z~x)  this is the error when thresholding the estimated p value to be smaller than 0.05, which is implemented via [stat] (see phase 2 of Alg 1 in the paper) ((a) in Figure 2)
3. P(Ep|z~x)   this is the EPV for hypotheses, also implemented via a binary search to the [stat] sequence (see phase 3) ((b) in Figure 2)


Coding Assets:
We refer to https://github.com/fengliu90/DK-for-TST for a similar implementation of learning kernels in the two-sample testing problem (we do not claim affiliation or responsibility to this github repository). For related benchmarks, we relied on codes from https://github.com/wittawatj/kernel-mod (UME) and https://github.com/aradha/recursive_feature_machines (RFM). For RFM specifically, while it is possible to construct a classifier out of a kernel, we only examine the kernel on MMD for our purposes.


