{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: value of $r$ is assigned in block [24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/math/home/eruisun/software/anaconda/envs/LFI/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "# from utils_old import *\n",
    "from utils import *\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from numba import cuda\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import pyroc\n",
    "import pandas as pd\n",
    "import gc\n",
    "from IPython.display import clear_output\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"  \n",
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float32\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import global_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 300\n",
    "out= 100\n",
    "L = 1\n",
    "class DN(torch.nn.Module):\n",
    "    def __init__(self, H=300, out=100):\n",
    "        super(DN, self).__init__()\n",
    "        self.restored = False\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, out, bias=True),\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        output = self.model(input)\n",
    "        return output\n",
    "\n",
    "class another_DN(torch.nn.Module):\n",
    "    def __init__(self, H=300, out=100):\n",
    "        super(another_DN, self).__init__()\n",
    "        self.restored = False\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28, H, bias=True),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(H, 28, bias=True),\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        output = self.model(input) + input\n",
    "        return output\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, H=300, layers = 5):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.restored = False\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, 1, bias=True),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "        if layers == 6:\n",
    "            self.model = torch.nn.Sequential(\n",
    "                torch.nn.Linear(28, H, bias=True),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(H, H, bias=True),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(H, H, bias=True),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(H, H, bias=True),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(H, H, bias=True),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(H, 1, bias=True),\n",
    "                torch.nn.Sigmoid(),\n",
    "            )\n",
    "    def forward(self, input):\n",
    "        output = self.model(input)\n",
    "        return output\n",
    "dataset = np.load('HIGGS.npy')\n",
    "dataset_P = dataset[dataset[:,0]==0][:, 1:] # background (5829122, 28)\n",
    "dataset_Q = dataset[dataset[:,0]==1][:, 1:] # signal     (5170877, 28)\n",
    "del dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creat class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_PQ(X_test, Y_test, model, another_model, epsilonOPT, sigmaOPT, sigma0OPT, cst,\n",
    "                n_test, Samples, batch_size = 5000, If_n_large_MonteCarlo = 1000, test_batch_size=20000):\n",
    "    '''\n",
    "    Input: \n",
    "        n_eval: n_eval in our paper, then we generate $X^{eval}$ and $Y^{eval}$ used to compute MMD.\n",
    "        Samples: we generate 'Samples' samples from P and Q, then compute their scores f(sample;X_eval,Y_eval)\n",
    "    Output:\n",
    "        X_eval, Y_eval: generate from P and Q, each with size (n_eval, 28)\n",
    "        P_scores, Q_scores: scores of samples from P and Q, each with size (Samples,)\n",
    "        EKxx, EKyy, EKxy: expectation of K(X,X), K(Y,Y), K(X,Y), where X~P_X, Y~P_Y. Then the gamma in our paper can be computed from these three values.\n",
    "    '''\n",
    "    # X_test = dataset_P[np.random.choice(dataset_P.shape[0], n_test, replace=False)]\n",
    "    # Y_test = dataset_Q[np.random.choice(dataset_Q.shape[0], n_test, replace=False)]\n",
    "    EKxx, EKyy, EKxy = compute_gamma(X_test, Y_test, model, another_model, epsilonOPT, sigmaOPT, sigma0OPT, cst, MonteCarlo=If_n_large_MonteCarlo)\n",
    "    batches = (Samples-1)//batch_size + 1\n",
    "    P_scores = np.zeros(Samples)\n",
    "    Q_scores = np.zeros(Samples)\n",
    "    for i in trange(batches):\n",
    "        remain = batch_size\n",
    "        if i==batches-1:\n",
    "            remain = Samples - batch_size*(batches-1)\n",
    "        S_hand = np.concatenate((dataset_P[np.random.choice(dataset_P.shape[0], remain, replace=False)],\n",
    "                    dataset_Q[np.random.choice(dataset_Q.shape[0], remain, replace=False)]), axis=0)\n",
    "        S_hand = MatConvert(S_hand, device, dtype)\n",
    "        PQhat_hand = compute_score_func(S_hand, X_test, Y_test, \n",
    "                    model, another_model, epsilonOPT, sigmaOPT, sigma0OPT, cst,\n",
    "                    M = test_batch_size)\n",
    "        PQhat_hand = PQhat_hand.cpu().detach().numpy()\n",
    "        P_scores[i*batch_size: i*batch_size+remain] = PQhat_hand[:remain]\n",
    "        Q_scores[i*batch_size: i*batch_size+remain] = PQhat_hand[remain:]\n",
    "        del S_hand, PQhat_hand\n",
    "        gc.collect()\n",
    "    return X_test, Y_test, P_scores, Q_scores, EKxx, EKyy, EKxy\n",
    "\n",
    "class PQ_data():\n",
    "    '''\n",
    "    Define a class to compute the p-value, the Type I/II error.\n",
    "    '''\n",
    "    def __init__(self, P_scores, Q_scores, EKxx, EKyy, EKxy):\n",
    "        self.P_scores = P_scores\n",
    "        self.Q_scores = Q_scores\n",
    "        self.P_mean = np.mean(P_scores)\n",
    "        self.P_std = np.std(P_scores)\n",
    "        self.Q_mean = np.mean(Q_scores)\n",
    "        self.Q_std = np.std(Q_scores)\n",
    "        self.EKxx = EKxx\n",
    "        self.EKyy = EKyy\n",
    "        self.EKxy = EKxy\n",
    "    def pval_T_m_in_sigma(self, pi, m, use_gaussian, MonteCarlo):\n",
    "        '''\n",
    "        Compute the expected significance of discovery.\n",
    "        Input:\n",
    "            pi: the strength of the mixture in our paper.\n",
    "            m: the number of samples, i.e. the size of Z in our paper.\n",
    "            use_gaussian: if use Gaussian Approximation, then use_gaussian = True, else use_gaussian = False.\n",
    "            MonteCarlo: when use_gaussian = False, we use Monte Carlo to approximate the expectation.\n",
    "        Output:\n",
    "            p: the expected significance of discovery (in units of Gaussian sigma)\n",
    "        '''\n",
    "        T = pi*self.Q_mean + (1-pi)*self.P_mean\n",
    "        P_scores = self.P_scores\n",
    "        mean = self.P_mean\n",
    "        std = self.P_std\n",
    "        if m==1:\n",
    "            p = np.mean(P_scores > T)\n",
    "            p = -scipy.stats.norm.ppf(p)\n",
    "            self.p = p\n",
    "            return p\n",
    "        if use_gaussian:\n",
    "            p = (T-mean)/std*np.sqrt(m)\n",
    "        else:\n",
    "            T_mix_MonteCarlo_list = np.zeros(MonteCarlo)\n",
    "            for i in range(MonteCarlo):\n",
    "                idx = np.random.choice(P_scores.shape[0], m, replace=False)\n",
    "                T_mix_MonteCarlo_list[i] = np.mean(P_scores[idx])\n",
    "            p = np.mean(T_mix_MonteCarlo_list > T)\n",
    "            p = -scipy.stats.norm.ppf(p)\n",
    "        self.p = p\n",
    "        return p\n",
    "    def type_1_error_H0(self, pi, m, use_gaussian, MonteCarlo):\n",
    "        '''\n",
    "        Compute the estimated Type I error.\n",
    "        Input:\n",
    "            pi: the strength of the mixture in our paper.\n",
    "            m: the number of samples, i.e. the size of Z in our paper.\n",
    "            use_gaussian: if use Gaussian Approximation, then use_gaussian = True, else use_gaussian = False.\n",
    "            MonteCarlo: when use_gaussian = False, we use Monte Carlo to approximate the expectation.\n",
    "        Output:\n",
    "            type_1_error: the estimated Type I error.   \n",
    "        '''\n",
    "        P_scores = self.P_scores\n",
    "        Q_scores = self.Q_scores\n",
    "        mean = self.P_mean\n",
    "        std = self.P_std\n",
    "        P_mean = self.P_mean\n",
    "        P_std = self.P_std\n",
    "        Q_mean = self.Q_mean\n",
    "        gamma = self.EKxx*(pi/2-1) + self.EKxy*(1-pi) + self.EKyy*(pi/2)\n",
    "        #gamma = (pi/2)*Q_mean + (1-pi/2)*P_mean\n",
    "        self.gamma = gamma\n",
    "        if m==1:\n",
    "            type_1_error = np.mean(P_scores > gamma)\n",
    "            self.type_1_error = type_1_error\n",
    "            return type_1_error\n",
    "        if use_gaussian:\n",
    "            type_1_error = 1-scipy.stats.norm.cdf((gamma-mean)/std*np.sqrt(m))\n",
    "        else:\n",
    "            MonteCarlo_list = np.zeros(MonteCarlo)\n",
    "            for i in range(MonteCarlo):\n",
    "                idx = np.random.choice(P_scores.shape[0], m, replace=False)\n",
    "                MonteCarlo_list[i] = np.mean(P_scores[idx])\n",
    "            type_1_error = np.mean(MonteCarlo_list > gamma)\n",
    "            del MonteCarlo_list\n",
    "            gc.collect()\n",
    "        self.type_1_error = type_1_error\n",
    "        return type_1_error\n",
    "    def type_2_error_H1(self, pi, m, use_gaussian, MonteCarlo):\n",
    "        '''\n",
    "        Compute the estimated Type II error.\n",
    "        Input:\n",
    "            pi: the strength of the mixture in our paper.\n",
    "            m: the number of samples, i.e. the size of Z in our paper.\n",
    "            use_gaussian: if use Gaussian Approximation, then use_gaussian = True, else use_gaussian = False.\n",
    "            MonteCarlo: when use_gaussian = False, we use Monte Carlo to approximate the expectation.\n",
    "        Output:\n",
    "            type_2_error: the estimated Type II error.              \n",
    "        '''\n",
    "        P_scores = self.P_scores\n",
    "        Q_scores = self.Q_scores\n",
    "        P_mean = self.P_mean\n",
    "        P_std = self.P_std\n",
    "        Q_mean = self.Q_mean\n",
    "        Q_std = self.Q_std\n",
    "        gamma = self.EKxx*(pi/2-1) + self.EKxy*(1-pi) + self.EKyy*(pi/2)\n",
    "        #gamma = (pi/2)*Q_mean + (1-pi/2)*P_mean\n",
    "        self.gamma = gamma\n",
    "        if m==1:\n",
    "            type_2_error = np.mean(Q_scores < gamma)\n",
    "            self.type_2_error = type_2_error\n",
    "            return type_2_error\n",
    "        if use_gaussian:\n",
    "            mean = Q_mean*pi + P_mean*(1-pi)\n",
    "            std = np.sqrt(pi*Q_std**2 + (1-pi)*P_std**2 + pi*(1-pi)*(P_mean-Q_mean)**2)\n",
    "            type_2_error = scipy.stats.norm.cdf((gamma-mean)/std*np.sqrt(m))\n",
    "        else:\n",
    "            MonteCarlo_list = np.zeros(MonteCarlo)\n",
    "            for i in range(MonteCarlo):\n",
    "                Signals_idx = np.random.choice(Q_scores.shape[0], int(m*pi), replace=False)\n",
    "                Backgrounds_idx = np.random.choice(P_scores.shape[0], int(m*(1-pi)), replace=False)\n",
    "                MonteCarlo_list[i] = np.mean(np.concatenate((Q_scores[Signals_idx], P_scores[Backgrounds_idx])))\n",
    "            type_2_error = np.mean(MonteCarlo_list < gamma)\n",
    "            del MonteCarlo_list\n",
    "            gc.collect()\n",
    "        self.type_2_error = type_2_error\n",
    "        return type_2_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate distribution of scores for a given $n_{train}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = np.array([2000000, 1600000, 1300000, 1000000, 700000, 400000, 200000, 100000, 50000, 30000, 10000, 6000, 3000, 1000, 600, 300])\n",
    "ns = np.array([1600000, 1300000, 1000000, 700000, 400000, 200000, 100000, 50000, 30000, 10000, 8000, 6000, 4500, 3000, 2000, 1000])\n",
    "ns = global_config.test_param_configs['n_tr_list']\n",
    "pi=0.1\n",
    "gc.collect()\n",
    "torch. cuda. empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given $r$ below, and some $n$, we use the checkpoint path = \"./Res_Net/checkpoint $(n+r)$/0/\"  \n",
    "To get a smooth plot, one should change the value of $r$ from 0 to 9, and rerun the whole .ipynb 10 times manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0 # For a given n, we use the checkpoint of the r-th trained network\n",
    "repeats_per_trained_model = 50 # for each trained network, we repeat the experiment this many times\n",
    "My_class_list = [[]]*repeats_per_trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 50000\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Res_Net/checkpoint50000/0/model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[39m=\u001b[39m DN(\u001b[39m300\u001b[39m, \u001b[39m100\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m      6\u001b[0m another_model \u001b[39m=\u001b[39m another_DN(\u001b[39m300\u001b[39m, \u001b[39m100\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m----> 7\u001b[0m model,another_model,epsilonOPT,sigmaOPT,sigma0OPT,cst \u001b[39m=\u001b[39m load_model(model, another_model, path)\n\u001b[1;32m      9\u001b[0m n_train \u001b[39m=\u001b[39m n\n\u001b[1;32m     10\u001b[0m n_test \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n,\u001b[39m20000\u001b[39m)\n",
      "File \u001b[0;32m~/github/LFI/utils.py:269\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model, another_model, path)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[39mreturn\u001b[39;00m model, \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mFea_Gau\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m, sigma0OPT, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(path\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmodel.pt\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m    270\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         another_model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(path\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39manother_model.pt\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/software/anaconda/envs/LFI/lib/python3.9/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/software/anaconda/envs/LFI/lib/python3.9/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/software/anaconda/envs/LFI/lib/python3.9/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Res_Net/checkpoint50000/0/model.pt'"
     ]
    }
   ],
   "source": [
    "for k in range(repeats_per_trained_model):\n",
    "    for i,n in enumerate(ns):\n",
    "        print(k,n)\n",
    "        path = './Res_Net/checkpoint%d/0/'%(n+r)\n",
    "        model = DN(300, 100).cuda()\n",
    "        another_model = another_DN(300, 100).cuda()\n",
    "        model,another_model,epsilonOPT,sigmaOPT,sigma0OPT,cst = load_model(model, another_model, path)\n",
    "\n",
    "        n_train = n\n",
    "        n_test = min(n,20000)\n",
    "        n_eval = 20000\n",
    "        idx = np.random.choice(dataset_P.shape[0]-n_train, n_eval+n_test, replace=False) + n_train\n",
    "        idy = np.random.choice(dataset_Q.shape[0]-n_train, n_eval+n_test, replace=False) + n_train\n",
    "        X_test = dataset_P[np.random.choice(n_train, n_test, replace=False)]\n",
    "        Y_test = dataset_Q[np.random.choice(n_train, n_test, replace=False)]\n",
    "        X_eval = dataset_P[idx][n_test:n_test+n_eval]\n",
    "        Y_eval = dataset_Q[idy][n_test:n_test+n_eval]\n",
    "        X_test = MatConvert(X_test, device=device, dtype=dtype)\n",
    "        Y_test = MatConvert(Y_test, device=device, dtype=dtype)\n",
    "        X_eval = MatConvert(X_eval, device=device, dtype=dtype)\n",
    "        Y_eval = MatConvert(Y_eval, device=device, dtype=dtype)\n",
    "        X_test_eval = X_test\n",
    "        Y_test_eval = Y_test\n",
    "\n",
    "        # p_soft = utils.get_pval_at_once(X_test, Y_test, X_test_eval, Y_test_eval, X_eval, Y_eval,\n",
    "        #                     model,another_model,epsilonOPT,sigmaOPT,sigma0OPT,cst,\n",
    "        #                     batch_size = 5000,\n",
    "        #                     norm_or_binom=True)\n",
    "        batch_size = 10000\n",
    "        Samples = 10000\n",
    "        If_n_large_MonteCarlo = 10\n",
    "        X_test, Y_test, P_scores, Q_scores, EKxx, EKyy, EKxy = generate_PQ(X_test, Y_test, model, another_model, epsilonOPT,sigmaOPT, sigma0OPT, cst,\n",
    "                                                                        n_test, Samples=Samples, batch_size=2000, \n",
    "                                                                        If_n_large_MonteCarlo=10,\n",
    "                                                                        test_batch_size=min(n,20000))\n",
    "        My_class  = PQ_data(P_scores, Q_scores, EKxx, EKyy, EKxy)\n",
    "        My_class_list[k].append(My_class)\n",
    "        del My_class\n",
    "        gc.collect()\n",
    "        clear_output()\n",
    "        print(k,n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate error for different $m$\n",
    "Note: one can adjust $ms$, the list of $m$ to calculate. But please make sure that generate_error_m_n_train.ipynb and plot_error_m_n_train.ipynb are using the same $ms$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My_class_list100 = My_class_list\n",
    "# error_mat = type1_mat+type2_mat\n",
    "# Error_m_ntr = np.mean(type1_mat+type2_mat, axis=2)\n",
    "# Error_m_ntr = error_mat[:,:,1]\n",
    "MonteCarlo = 1000\n",
    "step=5/49 # this can be changed\n",
    "new_step = 3*step # this can be changed\n",
    "ms = 10**np.linspace(0, 5, 50) # this can be changed\n",
    "ms = ms.astype(int)\n",
    "type1_mat_M = np.zeros((len(ns), len(ms), repeats_per_trained_model))\n",
    "type2_mat_M = np.zeros((len(ns), len(ms), repeats_per_trained_model))\n",
    "Error_m_ntr_M = np.zeros((len(ns), len(ms)))\n",
    "type1_mat_G = np.zeros((len(ns), len(ms), repeats_per_trained_model))\n",
    "type2_mat_G = np.zeros((len(ns), len(ms), repeats_per_trained_model))\n",
    "Error_m_ntr_G = np.zeros((len(ns), len(ms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(repeats_per_trained_model):\n",
    "    for i,n in enumerate(ns):\n",
    "        My_class = My_class_list[k][i]\n",
    "        #My_class_list[k].append(My_class)\n",
    "        MonteCarlo = 1000\n",
    "        for j in range(len(ms)):\n",
    "            print(i,j,k)\n",
    "            m = ms[j]\n",
    "            #P_m_ntr[i,j] += p_soft*11/np.sqrt(1100) * pi * np.sqrt(m)\n",
    "            # type1 = My_class.type_1_error_H0(pi, m, use_gaussian=True, MonteCarlo=MonteCarlo)\n",
    "            # type2 =  My_class.type_2_error_H1(pi, m, use_gaussian=True, MonteCarlo=MonteCarlo)\n",
    "            # type1_mat_G[i,j,k] = type1\n",
    "            # type2_mat_G[i,j,k] = type2\n",
    "            # Error_m_ntr_G[i,j] += (type2 + type1)\n",
    "            if m< 1000:\n",
    "                type1 = My_class.type_1_error_H0(pi, m, use_gaussian=False, MonteCarlo=MonteCarlo) \n",
    "                type2 =  My_class.type_2_error_H1(pi, m, use_gaussian=False, MonteCarlo=MonteCarlo)\n",
    "                type1_mat_M[i,j,k] = type1\n",
    "                type2_mat_M[i,j,k] = type2\n",
    "                Error_m_ntr_M[i,j] += (type2 + type1)\n",
    "            else:\n",
    "                type1 = My_class.type_1_error_H0(pi, m, use_gaussian=True, MonteCarlo=MonteCarlo)\n",
    "                type2 =  My_class.type_2_error_H1(pi, m, use_gaussian=True, MonteCarlo=MonteCarlo)\n",
    "                type1_mat_M[i,j,k] = type1\n",
    "                type2_mat_M[i,j,k] = type2\n",
    "                Error_m_ntr_M[i,j] += (type2 + type1)\n",
    "                \n",
    "        del My_class\n",
    "        gc.collect()\n",
    "        clear_output()\n",
    "#P_m_ntr = P_m_ntr/repeats\n",
    "# Error_m_ntr = Error_m_ntr/repeats\n",
    "Error_m_ntr_M = Error_m_ntr_M/repeats_per_trained_model\n",
    "Error_m_ntr_G = Error_m_ntr_G/repeats_per_trained_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_str = str(r)\n",
    "np.save('ns'+r_str, ns)\n",
    "np.save('ms'+r_str, ms)\n",
    "np.save('Error_m_ntr_M'+r_str, Error_m_ntr_M)\n",
    "# np.save('Error_m_ntr_G'+r_str, Error_m_ntr_M)\n",
    "# np.save('type1_mat_M'+r_str, type1_mat_M)\n",
    "# np.save('type2_mat_M'+r_str, type2_mat_M)\n",
    "# np.save('type1_mat_G'+r_str, type1_mat_M)\n",
    "# np.save('type2_mat_G'+r_str, type2_mat_M)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(5,4))\n",
    "# plt.contourf(np.log10(ns), np.log10(ms), Error_m_ntr_M.T, levels=20)\n",
    "# plt.colorbar()\n",
    "# plt.xlabel('$lg(n)$')\n",
    "# plt.ylabel('lg(m)')\n",
    "# plt.title('Type I error + Type II error (this is not the formal plot)')\n",
    "\n",
    "# #plt.suptitle(r'Fix kernel ($n_{train}=1.3e6$), Use Gaussian Approx='+str(Use_Gaussian)+', $\\pi$='+str(pi))\n",
    "# # plt.savefig('./paper/tradeoff_type.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LFI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3398c4b5aa1e55195b0bb96b4f8fc5c3f4a0ffb5794362f58b6653d525fb08f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
